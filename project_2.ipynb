{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import stanza\n",
    "import nltk\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = []\n",
    "for num in range(1, 1069):\n",
    "    main.append(f'https://shikimori.one/animes/page/{num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = rq.get(main[0], headers={'User-Agent': UserAgent().chrome})\n",
    "print(page.status_code)\n",
    "soup = BeautifulSoup(page.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [url.get('href') for url in soup.find_all('a')]\n",
    "\n",
    "page_links = []\n",
    "for link in links:\n",
    "    if 'https://shikimori.one/animes/' in str(link) and 'https://shikimori.one/animes/page/' not in str(link):\n",
    "        page_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_links(list):\n",
    "    mistakes = []\n",
    "    page_links = []\n",
    "\n",
    "    for i in list:\n",
    "        page = rq.get(i, headers={'User-Agent': UserAgent().chrome})\n",
    "        if page.status_code != 200:\n",
    "            mistakes.append(i)\n",
    "        else:\n",
    "            soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "            links = [url.get('href') for url in soup.find_all('a')]\n",
    "\n",
    "            for link in links:\n",
    "                if 'https://shikimori.one/animes/' in str(link) and 'https://shikimori.one/animes/page/' not in str(link):\n",
    "                    page_links.append(link)\n",
    "    \n",
    "    return [mistakes, page_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mistakes = []\n",
    "all_page_links = []\n",
    "\n",
    "mistakes, page_links = collect_links(main)\n",
    "\n",
    "all_mistakes.extend(mistakes)\n",
    "all_page_links.extend(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while all_mistakes:\n",
    "    new_mistakes, new_page_links = collect_links(all_mistakes)\n",
    "    all_mistakes = []\n",
    "    all_mistakes.extend(new_mistakes)\n",
    "    all_page_links.extend(new_page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_links = list(set(all_page_links))\n",
    "len(all_page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.txt', 'x') as file:\n",
    "    print(*all_page_links, file=file, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция-парсер\n",
    "\n",
    "def get_info(link):\n",
    "    page = rq.get(link, headers={'User-Agent': UserAgent().chrome})\n",
    "    soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "    \n",
    "    # название\n",
    "    try:\n",
    "        name_ru, name_jp = soup.find('h1').text.split(' / ')\n",
    "    except:\n",
    "        name_jp = soup.find('h1').text\n",
    "        name_ru = None\n",
    "    \n",
    "    # описание\n",
    "    try:\n",
    "        description = soup.find('div', class_='c-description').find('div', class_='b-text_with_paragraphs').text\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    # рейтинг\n",
    "    try:\n",
    "        rating = soup.find('div', class_='c-info-right').find('div', class_='scores').find(itemprop='ratingValue').get('content')\n",
    "    except:\n",
    "        rating = None\n",
    "        \n",
    "    # тип\n",
    "    info = soup.find('div', class_='c-about').find_all('div', class_='value')\n",
    "    type = info[0].text\n",
    "\n",
    "    # студия\n",
    "    links = [url.get('href') for url in soup.find_all('a')]\n",
    "    studio = None\n",
    "    for l in links:\n",
    "        if 'studio/' in str(l):\n",
    "            studio = str(l).split('studio/')[1]\n",
    "\n",
    "    # жанры\n",
    "    genre_check, genres = [], []\n",
    "    for i in info:\n",
    "        genre_check.extend(i.find_all('span', class_='genre-ru'))\n",
    "    for j in genre_check:\n",
    "        genres.append(j.text.lower())\n",
    "    \n",
    "    #кол-во и длина эпизодов \n",
    "    ep_len = soup.find('div', class_='c-about').find('span', class_=None).text\n",
    "    try:\n",
    "        ep = int(info[1].text)\n",
    "    except:\n",
    "        if '/' in info[1].text:\n",
    "            ep = info[1].text\n",
    "        else:\n",
    "            ep = 1\n",
    "    \n",
    "    # статус\n",
    "    ongoing = True if '/' in info[1].text else False\n",
    "\n",
    "    # возрастные ограничения, дата выхода и окончания\n",
    "    date_age = soup.find('div', class_='c-about').find_all('span', class_='b-tooltipped dotted mobile unprocessed')\n",
    "    \n",
    "    try:\n",
    "        if len(date_age) == 1:\n",
    "            age_rating = date_age[0].text\n",
    "            date = info[4].text if ongoing else info[2].text\n",
    "        else:\n",
    "            date, age_rating = date_age[0].text, date_age[1].text\n",
    "    except:\n",
    "        date, age_rating = None, None\n",
    "    \n",
    "    try:\n",
    "        if 'по' in date:\n",
    "            start_date, end_date = date.split(' по ')\n",
    "            start_date = start_date.replace('\\xa0с ', '')\n",
    "        elif '-' in date:\n",
    "            start_date, end_date = date.split('-')\n",
    "            start_date = start_date.replace('\\xa0в ', '')\n",
    "            end_date = end_date.replace(' гг.', '')\n",
    "        else:\n",
    "            #start_date = date.replace('\\xa0', '')\n",
    "            start_date = date.replace('\\xa0с ', '') if ongoing == True else date.replace('\\xa0', '')\n",
    "            end_date = None\n",
    "    except:\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "\n",
    "\n",
    "    information = {\n",
    "        'name_ru': name_ru,\n",
    "        'name_jp': name_jp,\n",
    "        'type': type,\n",
    "        'ep': ep,\n",
    "        'ep_len': ep_len,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'ongoing': ongoing,\n",
    "        'genres': genres,\n",
    "        'age_rating': age_rating,\n",
    "        'studio': studio,\n",
    "        'rating': rating,\n",
    "        'description': description}\n",
    "    \n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.txt', 'r') as file:\n",
    "    links = file.read()\n",
    "\n",
    "links_to_parse = links.split('\\n')\n",
    "\n",
    "mistakes = []\n",
    "parsed = []\n",
    "\n",
    "for link in links_to_parse:\n",
    "    try:\n",
    "        parsed.append(get_info(link))\n",
    "    except:\n",
    "        mistakes.append(link)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# помечено как 18+ и заблокировано - добавляю информацию вручную\n",
    "# описание из манги (первоисточника сериала) с этого же сайта: https://shikimori.one/mangas/z21-death-note\n",
    "\n",
    "death_note = {\n",
    "        'name_ru': 'Тетрадь смерти',\n",
    "        'name_jp': 'Death Note',\n",
    "        'type': 'TV Сериал',\n",
    "        'ep': 37,\n",
    "        'ep_len': '23 мин.',\n",
    "        'start_date': 2006,\n",
    "        'end_date': 2007,\n",
    "        'ongoing': False,\n",
    "        'genres': ['сёнен', 'сверхъестественное', 'триллер', 'психологическое'],\n",
    "        'age_rating': 'R-17',\n",
    "        'studio': '11-Madhouse',\n",
    "        'rating': '8.62',\n",
    "        'description': 'Лайт Ягами — образцовый 17-летний выпускник, баллы за экзамены которого находятся в первых строках рейтинга всей Японии. Сидя на уроке, он замечает, что за окном что-то упало. На перемене он поднимает загадочный предмет и им оказывается черная тетрадь с надписью «Тетрадь смерти». Внутри была инструкция по использованию: \"Человек, имя которого будет записано в тетради, умрет\". Имея свои взгляды на систему наказания, Лайт решает установить собственное правосудие, использовать тетрадь для «очищения» мира от зла — убивать преступников. Когда действия Лайта становятся заметны для мирового правительства, на след неуловимого «Киры» (так мир окрестил нового мессию, решившего искоренить зло на планете) выходит детектив мирового класса, называющий себя «L», который поставил себе целью разоблачить убийцу. Так начинается одно из самых психологичных, напряженных и сильных противостояний в истории японской манги, величайшая битва умов.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mistakes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в остальных 6 случаях ошибка возникала из-за отсутствия информации о длительности эпизода\n",
    "# ниже та же функция, но длительность эпизода сразу задана None\n",
    "\n",
    "def get_info_mistakes(link):\n",
    "    page = rq.get(link, headers={'User-Agent': UserAgent().chrome})\n",
    "    soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "    \n",
    "    # название\n",
    "    try:\n",
    "        name_ru, name_jp = soup.find('h1').text.split(' / ')\n",
    "    except:\n",
    "        name_jp = soup.find('h1').text\n",
    "        name_ru = None\n",
    "    \n",
    "    # описание\n",
    "    try:\n",
    "        description = soup.find('div', class_='c-description').find('div', class_='b-text_with_paragraphs').text\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    # рейтинг\n",
    "    try:\n",
    "        rating = soup.find('div', class_='c-info-right').find('div', class_='scores').find(itemprop='ratingValue').get('content')\n",
    "    except:\n",
    "        rating = None\n",
    "        \n",
    "    # тип\n",
    "    info = soup.find('div', class_='c-about').find_all('div', class_='value')\n",
    "    type = info[0].text\n",
    "\n",
    "    # студия\n",
    "    links = [url.get('href') for url in soup.find_all('a')]\n",
    "    studio = None\n",
    "    for l in links:\n",
    "        if 'studio/' in str(l):\n",
    "            studio = str(l).split('studio/')[1]\n",
    "\n",
    "    # жанры\n",
    "    genre_check, genres = [], []\n",
    "    for i in info:\n",
    "        genre_check.extend(i.find_all('span', class_='genre-ru'))\n",
    "    for j in genre_check:\n",
    "        genres.append(j.text.lower())\n",
    "    \n",
    "    #кол-во и длина эпизодов \n",
    "    ep_len = None\n",
    "    try:\n",
    "        ep = int(info[1].text)\n",
    "    except:\n",
    "        if '/' in info[1].text:\n",
    "            ep = info[1].text\n",
    "        else:\n",
    "            ep = 1\n",
    "    \n",
    "    # статус\n",
    "    ongoing = True if '/' in info[1].text else False\n",
    "\n",
    "    # возрастные ограничения, дата выхода и окончания\n",
    "    date_age = soup.find('div', class_='c-about').find_all('span', class_='b-tooltipped dotted mobile unprocessed')\n",
    "    \n",
    "    try:\n",
    "        if len(date_age) == 1:\n",
    "            age_rating = date_age[0].text\n",
    "            date = info[4].text if ongoing else info[2].text\n",
    "        else:\n",
    "            date, age_rating = date_age[0].text, date_age[1].text\n",
    "    except:\n",
    "        date, age_rating = None, None\n",
    "    \n",
    "    try:\n",
    "        if 'по' in date:\n",
    "            start_date, end_date = date.split(' по ')\n",
    "            start_date = start_date.replace('\\xa0с ', '')\n",
    "        elif '-' in date:\n",
    "            start_date, end_date = date.split('-')\n",
    "            start_date = start_date.replace('\\xa0в ', '')\n",
    "            end_date = end_date.replace(' гг.', '')\n",
    "        else:\n",
    "            start_date = date.replace('\\xa0с ', '') if ongoing == True else date.replace('\\xa0', '')\n",
    "            end_date = None\n",
    "    except:\n",
    "        start_date = None\n",
    "        end_date = None\n",
    "\n",
    "\n",
    "    information = {\n",
    "        'name_ru': name_ru,\n",
    "        'name_jp': name_jp,\n",
    "        'type': type,\n",
    "        'ep': ep,\n",
    "        'ep_len': ep_len,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'ongoing': ongoing,\n",
    "        'genres': genres,\n",
    "        'age_rating': age_rating,\n",
    "        'studio': studio,\n",
    "        'rating': rating,\n",
    "        'description': description}\n",
    "    \n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_2 = [get_info_mistakes(page) for page in mistakes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_2.append(death_note)\n",
    "df_2 = pd.DataFrame(parsed_2)\n",
    "df_2['start_date'][2:5] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# я решила добавить ссылки (к сожалению, не догадалась добавить это в функцию сразу)\n",
    "\n",
    "links_2 = ['https://shikimori.one/animes/56624-araburu-kisetsu-no-otome-domo-yo-mini-anime',\n",
    "'https://shikimori.one/animes/55566-wasted-chef',\n",
    "'https://shikimori.one/animes/49941-gundam-uc-x-nike-sb',\n",
    "'https://shikimori.one/animes/42748-pure-shield',\n",
    "'https://shikimori.one/animes/33312-color-noise',\n",
    "'https://shikimori.one/animes/54160-future-kid-takara',\n",
    "'https://shikimori.one/animes/1535-death-note']\n",
    "\n",
    "df_2['link'] = links_2\n",
    "\n",
    "links_1 = [l for l in links_to_parse if l not in links_2]\n",
    "df_1['link'] = links_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2])\n",
    "df.to_csv('anime_info.csv', sep = ',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## очистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anime_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1)\n",
    "df = df[df['type'] != 'Реклама']\n",
    "df = df[df['type'] != 'Проморолик']\n",
    "df = df.dropna(subset = ['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# год начала выхода\n",
    "\n",
    "start_date = df['start_date'].fillna(0).tolist()\n",
    "start_year = []\n",
    "start_year_mistakes = []\n",
    "\n",
    "for i in range(len(start_date)):\n",
    "    try:\n",
    "        res = int(re.search(r'\\d{4}', start_date[i]).group())\n",
    "        start_year.append(res)\n",
    "    except:\n",
    "        start_year.append(None)\n",
    "        start_year_mistakes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# год конца выхода\n",
    "\n",
    "ep = df['ep'].tolist()\n",
    "ongoing = df['ongoing'].tolist()\n",
    "\n",
    "end_date = df['end_date'].fillna(0).tolist()\n",
    "end_year = []\n",
    "end_year_mistakes = []\n",
    "\n",
    "\n",
    "for i in range(len(end_date)):\n",
    "    if end_date[i] == 0:\n",
    "        if ep[i] in [1, '1']:\n",
    "            end_year.append(start_year[i])\n",
    "        elif ongoing[i] == True:\n",
    "            end_year.append(2024)\n",
    "        else:\n",
    "            end_year.append(None)\n",
    "            end_year_mistakes.append(i)\n",
    "    else:\n",
    "        try:\n",
    "            res = int(re.search(r'\\d{4}', end_date[i]).group())\n",
    "            end_year.append(res)\n",
    "        except:\n",
    "            end_year.append(None)\n",
    "            end_year_mistakes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_date'] = start_year \n",
    "df['end_date'] = end_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код для ручной проверки дат, которые попали в ошибки\n",
    "\n",
    "all_date_mistakes = list(set(end_year_mistakes) | set(start_year_mistakes))\n",
    "\n",
    "all_date_clean = pd.DataFrame(start_year)\n",
    "all_date_clean['1'] = end_year\n",
    "\n",
    "df_date_check = df.iloc[all_date_mistakes, [0, 5, 6, 13]]\n",
    "df_date_check['start_date_new'] = all_date_clean.iloc[all_date_mistakes, 0]\n",
    "df_date_check['end_date_new'] = all_date_clean.iloc[all_date_mistakes, 1]\n",
    "df_date_check['index'] = all_date_mistakes\n",
    "\n",
    "df_date_check.to_excel('date_check.xlsx')\n",
    "\n",
    "date_check_links = df_date_check['link'].tolist()\n",
    "\n",
    "with open('date_check_links.txt', 'x') as file:\n",
    "    print(*date_check_links, file=file, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у многих работ указана дата начала, поэтому попробую спарсить еще раз\n",
    "\n",
    "parsing_date_again = []\n",
    "for d in date_check_links:\n",
    "    page = rq.get(d, headers={'User-Agent': UserAgent().chrome})\n",
    "    soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "    try:\n",
    "        date = soup.find('div', class_='c-about').find_all('div', class_='value')[3].text\n",
    "        parsing_date_again.append(int(re.search(r'\\d{4}', date).group()))\n",
    "    except:\n",
    "        parsing_date_again.append(None)\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так как в список ошибок попали в основном короткие работы, год конца выхода будет таким же, как и год начала\n",
    "\n",
    "df_date_clean = df.iloc[all_date_mistakes]\n",
    "df_date_clean['start_date'] = parsing_date_again\n",
    "df_date_clean['end_date'] = parsing_date_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[all_date_mistakes], axis=0, inplace=True)\n",
    "df_upd = pd.concat([df, df_date_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# студия\n",
    "\n",
    "studio_dirty = df_upd['studio'].tolist()\n",
    "studio_clean = []\n",
    "\n",
    "for i in range(len(studio_dirty)):\n",
    "    try:\n",
    "        studio_clean.append(re.sub(r'\\d{1,}-', '', studio_dirty[i]))\n",
    "    except:\n",
    "        studio_clean.append(studio_dirty[i])\n",
    "\n",
    "df_upd['studio'] = studio_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upd.to_csv('anime_info_upd.csv', sep = ',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anime_info_upd.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-во эпизодов: изменение формата для онгоингов с \"12 / 24\" или \"100 / ?\" на одно число\n",
    "\n",
    "ep_dirty = df['ep'].tolist()\n",
    "ep_clean = []\n",
    "ep_mistakes = []\n",
    "\n",
    "for i in range(len(ep_dirty)):\n",
    "    try:\n",
    "        res = int(ep_dirty[i])\n",
    "    except:\n",
    "        if df.iloc[i, 7] == True:\n",
    "            res = int(ep_dirty[i].split(' / ')[0]) if '?' in ep_dirty[i] else int(ep_dirty[i].split(' / ')[1])\n",
    "        else:\n",
    "            res = ep_dirty[i]\n",
    "            ep_mistakes.append(i)\n",
    "    ep_clean.append(res)\n",
    "\n",
    "df['ep'] = ep_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возрастной рейтинг - в некоторые строки попали даты\n",
    "\n",
    "age_dirty = df['age_rating'].fillna(0).tolist()\n",
    "age_na = []\n",
    "age_mistakes = []\n",
    "\n",
    "for i in range(len(age_dirty)):\n",
    "    if age_dirty[i] not in ['G', 'PG', 'PG-13', 'R+', 'R-17']:\n",
    "        if age_dirty[i] == 0:\n",
    "            age_na.append(i)\n",
    "        else:\n",
    "            age_mistakes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_na = df.iloc[age_na]\n",
    "df_age = df.iloc[age_mistakes]\n",
    "\n",
    "age_date = df_age['age_rating'].tolist()\n",
    "date_new = [int(re.search(r'\\d{4}', i).group()) for i in age_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) год либо не указан, (2) либо указан правильно у работ с одинаковым годом начала и конца, (3) либо год конца указан неправильно (такой же, как год начала)\n",
    "# ошибку в третьем случае исправлю ниже\n",
    "\n",
    "df_age['start_date'], df_age['end_date'] = date_new, date_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ru = ['Викторина Токико', 'Волшебная улица Чанъань', 'Записи о даосском мече дождя и ветра', 'Обезьяний пик', 'Волейбольный клуб старшей школы Сэйин: Мини-аниме — Курсы по волейболу']\n",
    "\n",
    "manual_check = df_age.loc[df_age['name_ru'].isin(names_ru)]\n",
    "\n",
    "years = [2021, 2021, 2019, 2018, 2021]\n",
    "manual_check['end_date'] = years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убираю строки, которые исправила выше, и добавляю эти же исправленные строки\n",
    "\n",
    "index = df_age[df_age['name_ru'].isin(names_ru)].index\n",
    "df_age.drop(index, inplace=True)\n",
    "df_age = pd.concat([df_age, manual_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробую еще раз спарсить возрастной рейтинг\n",
    "\n",
    "df_wrong_age = pd.concat([df_age, df_age_na])\n",
    "links = df_wrong_age['link'].tolist()\n",
    "\n",
    "parsing_age_again = []\n",
    "none_links = []\n",
    "mistakes_links = []\n",
    "\n",
    "for l in links:\n",
    "    page = rq.get(l, headers={'User-Agent': UserAgent().chrome})\n",
    "    soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "    try:\n",
    "        age = soup.find('div', class_='c-about').find('span', class_='b-tooltipped dotted mobile unprocessed').text\n",
    "        if age in ['G', 'PG', 'PG-13', 'R+', 'R-17']:\n",
    "            parsing_age_again.append(age)\n",
    "        else:\n",
    "            parsing_age_again.append(None)\n",
    "            mistakes_links.append(l)\n",
    "    except:\n",
    "        parsing_age_again.append(None)\n",
    "        none_links.append(l)\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# я проверила эти ссылки вручную - возрастной рейтинг действительно не указан\n",
    "\n",
    "with open('age_na_links.txt', 'x') as file:\n",
    "    print(*none_links, file=file, sep='\\n')\n",
    "\n",
    "# у работ, из которых вместо рейтинга подтягивалась дата, тоже не указан рейтинг\n",
    "\n",
    "with open('age_mistakes_links.txt', 'x') as file:\n",
    "    print(*mistakes_links, file=file, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wrong_age['age_rating'] = parsing_age_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для замены части датасета\n",
    "\n",
    "def replace_rows(df_main, df_new_rows):\n",
    "    names = df_new_rows['name_jp'].tolist()\n",
    "    index = df_main[df_main['name_jp'].isin(names)].index\n",
    "    df = df_main.drop(index)\n",
    "    return pd.concat([df, df_new_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upd = replace_rows(df, df_wrong_age)\n",
    "df_upd.to_csv('anime_info_upd_2.csv', sep = ',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anime_info_upd_2.csv', converters={'genres': pd.eval})\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в колонке с длительностью эпизода должно остаться только общее количество минут\n",
    "\n",
    "df['ep_len'] = np.where((df.ep_len == '···'), None, df.ep_len)\n",
    "ep_len = df['ep_len'].fillna('').tolist()\n",
    "ep_len_new = []\n",
    "ep_len_mistakes = []\n",
    "\n",
    "for i in range(len(ep_len)):\n",
    "    if 'час' in ep_len[i] and 'мин' in ep_len[i]:\n",
    "        res = 0\n",
    "        hour, min = ep_len[i].split(' ч')\n",
    "        min = int(re.search(r'\\d+', min).group())\n",
    "        res += int(hour) * 60 + min\n",
    "        ep_len_new.append(res)\n",
    "    elif 'час' in ep_len[i]:\n",
    "        res = 0\n",
    "        hour = ep_len[i].split(' ч')\n",
    "        res += int(hour[0]) * 60\n",
    "        ep_len_new.append(res)\n",
    "    else:\n",
    "        try:\n",
    "            res = int(re.search(r'\\d+', ep_len[i]).group())\n",
    "            ep_len_new.append(res)\n",
    "        except:\n",
    "            ep_len_new.append(None)\n",
    "            ep_len_mistakes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверила эти ссылки вручную - длительность эпизода нигде не указана, в датасете останется None\n",
    "\n",
    "links = df.iloc[ep_len_mistakes]['link'].tolist() \n",
    "with open('ep_len_links.txt', 'x') as file:\n",
    "    print(*links, file=file, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ep_len'] = ep_len_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у всех онгоингов год конца выхода должен быть указан 2024\n",
    "\n",
    "df_ongoing = df.loc[df['ongoing'] == True]\n",
    "df_ongoing['end_date'] = 2024\n",
    "df = replace_rows(df, df_ongoing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у некоторых работ не указано название на русском\n",
    "# сначала исправлю вручную то, что не потребуется переводить\n",
    "\n",
    "df.loc[df['name_jp'] == 'Детектив Холмс: Дело о Голубом Рубине/Дело о сокровищах со дна моря / Meitantei Holmes: Aoi Ruby no Maki / Kaitei no Zaihou no Maki', 'name_jp'] = 'Meitantei Holmes: Aoi Ruby no Maki / Kaitei no Zaihou no Maki'\n",
    "df.loc[df['name_jp'] == 'Meitantei Holmes: Aoi Ruby no Maki / Kaitei no Zaihou no Maki', 'name_ru'] = 'Детектив Холмс: Дело о Голубом Рубине/Дело о сокровищах со дна моря'\n",
    "\n",
    "df.loc[df['name_jp'] == '663114', 'name_ru'] = '663114'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь соберу ссылки на оставшиеся работы без названия на русском, переведу названия в онлайн-переводчике\n",
    "\n",
    "df_no_name = df.loc[df['name_ru'].isna()]\n",
    "links = df_no_name['link'].tolist()\n",
    "with open('no_name_links.txt', 'x') as file:\n",
    "    print(*links, file=file, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# когда я проверяла ссылки, в описании одной из работ прочитала, что это реклама, поэтому удаляю\n",
    "\n",
    "df_no_name.drop(df_no_name.loc[df_no_name['name_jp'] == 'Wares: Beyond'].index, inplace=True)\n",
    "df.drop(df.loc[df['name_jp'] == 'Wares: Beyond'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_name['name_jp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ru_translated = ['Стоп! Игра с зажигалкой: Команда тушения деревни животных Дейдо',\n",
    "'Третий лишний', \n",
    "'Безопасность дорожного движения в городе Синсэнгуми Отасукегуми Аймедзаси-тай. Стремитесь соблюдать правила дорожного движения! Зоопарк!',\n",
    "'Опетте',\n",
    "'Издевательства - это абсолютно неправильно!',\n",
    "'Гимнастика в соответствии с руководством по скринингу опорно-двигательного аппарата',\n",
    "'Исполнительный комитет',\n",
    "'Я немедленно сбежал: чему меня научило Великое землетрясение на востоке Японии',\n",
    "'Матиерика',\n",
    "'Все в другом мире — призраки!',\n",
    "'Знаете ли вы, как это больно: я не могу простить! Кибербуллинг',\n",
    "'Положи руку на грудь',\n",
    "'Начало дружбы',\n",
    "'Радужная связь',\n",
    "'Нана Мун',\n",
    "'Животные под угрозой исчезновения',\n",
    "'Однокрылый механизм Хроноса']\n",
    "\n",
    "df_no_name['name_ru'] = names_ru_translated\n",
    "df = replace_rows(df, df_no_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "index_drop = []\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i].isna().sum() > 3:\n",
    "        index_drop.append(i)\n",
    "\n",
    "df_upd = df.drop(index_drop, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upd['start_date'] = df_upd['start_date'].astype('int64')\n",
    "df_upd['end_date'] = df_upd['end_date'].astype('int64')\n",
    "df_upd['ep_len'] = df_upd['ep_len'].astype('int64')\n",
    "df_upd['rating'] = df_upd['rating'].astype('float')\n",
    "\n",
    "df = df_upd.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код для удаления года из названия - использовать только если будут проблемы при соединении с пользовательским датасетом\n",
    "\n",
    "#def name_check(name):\n",
    "#    return re.sub(r'[(]\\d{4}[)]', '', name)\n",
    "\n",
    "#df['name_jp'], df['name_ru'] = df['name_jp'].apply(name_check), df['name_ru'].apply(name_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = df['genres'].tolist()\n",
    "for list in all_genres:\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == 'cgdct':\n",
    "            list[i] = 'милые девушки делают милые вещи'\n",
    "    list.sort()\n",
    "\n",
    "df['genres'] = all_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код для просмотра всех встречающихся жанров\n",
    "\n",
    "genres = []\n",
    "for i in all_genres:\n",
    "    genres.extend(i)\n",
    "\n",
    "print(sorted(set(genres)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('anime_info_full.csv', sep = ',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обработка описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anime_info_full.csv', converters={'genres': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_description = []\n",
    "genres = df['genres']\n",
    "description = df['description']\n",
    "\n",
    "for row in range(len(df)):\n",
    "    res = ', '.join(genres[row]) + '\\t' + '\\n'*2 + description[row]\n",
    "    genres_description.append(res)\n",
    "\n",
    "df['genres_description'] = genres_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление имен из описаний и лемматизация\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ru', use_gpu=True)\n",
    "\n",
    "def clean_description(text):\n",
    "    doc = nlp(text)\n",
    "    no_names = []\n",
    "    for sentence in doc.sentences:\n",
    "        for t in sentence.tokens:\n",
    "            no_names.append('') if t.ner in ['B-PER', 'E-PER', 'S-PER', 'I-PER'] else no_names.append(t.text)\n",
    "\n",
    "    not_lemmatized = nlp(' '.join(no_names))\n",
    "    res = []\n",
    "    for sent in not_lemmatized.sentences:\n",
    "        lem_sent = [word.lemma for word in sent.words]\n",
    "        res.extend(lem_sent)\n",
    "        res.extend('\\n')\n",
    "\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_transformer = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "def sentence_embeddings(text):\n",
    "    clean_text = clean_description(text)\n",
    "    tok = nltk.sent_tokenize(clean_text)\n",
    "    embeddings = sent_transformer.encode(tok)\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_embeddings'] = df['genres_description'].apply(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('anime_info_emb.csv', sep = ',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если нужно открыть файл\n",
    "\n",
    "#df = pd.read_csv('anime_info_emb.csv')\n",
    "#df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "#df['sentence_embeddings'] = df['sentence_embeddings'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## пользовательские данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для обработки пользовательского датасета\n",
    "\n",
    "def concat_data(file_name):\n",
    "    df_user = pd.read_json(f'{file_name}.json')\n",
    "    df_user.drop(['target_id', 'target_type', 'rewatches', 'episodes', 'text'], axis=1, inplace=True)\n",
    "    df_user = df_user[df_user.status != 'planned']\n",
    "    df_user = df_user[df_user.status != 'on_hold']\n",
    "\n",
    "    status = df_user['status'].tolist()\n",
    "    score = df_user['score'].tolist()\n",
    "    for i in range(len(df_user)):\n",
    "        if status[i] == 'dropped' and score[i] == 0:\n",
    "            score[i] = -1\n",
    "    df_user['score'] = score\n",
    "    \n",
    "    df = pd.read_csv('anime_info_emb.csv')\n",
    "    df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "    df['sentence_embeddings'] = df['sentence_embeddings'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "\n",
    "    names_jp = df['name_jp'].tolist()\n",
    "    score = []\n",
    "    status = []\n",
    "\n",
    "    for name in names_jp:\n",
    "        try:\n",
    "            df_user.loc[df_user['target_title'] == name]\n",
    "            score.append(df_user.loc[df_user['target_title'] == name]['score'].item())\n",
    "            status.append(df_user.loc[df_user['target_title'] == name]['status'].item())\n",
    "        except:\n",
    "            score.append(0)\n",
    "            status.append('not_watched')\n",
    "\n",
    "    df['user_score'] = score\n",
    "    df['user_status'] = status\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_to_user будет учитывать все просмотренные работы\n",
    "\n",
    "#def user_profile(file_name):\n",
    "#    df = concat_data(file_name)\n",
    "#    df_watched = df[df.user_status == 'completed']\n",
    "#    emb = df_watched['sentence_embeddings'].tolist()\n",
    "#    return np.mean(emb, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_to_user будет рассчитана только по работам с оценкой не ниже средней оценки пользователя\n",
    "# (если просмотрено больше 300 работ и не менее 100 из них оценены)\n",
    "\n",
    "def user_profile(file_name):\n",
    "    df = concat_data(file_name)\n",
    "    df_watched = df[df.user_status == 'completed']\n",
    "    df_no_zero = df_watched[df_watched['user_score'] > 0]\n",
    "    \n",
    "    if len(df_watched) >= 300 and len(df_no_zero) >= 100:\n",
    "        user_score = df_no_zero['user_score'].tolist()\n",
    "        average = (sum(user_score) / float(len(user_score))) // 1\n",
    "        df_high_score = df_watched[df_watched['user_score'] >= average]\n",
    "        emb = df_high_score['sentence_embeddings'].tolist()\n",
    "    else:\n",
    "        emb = df_watched['sentence_embeddings'].tolist()\n",
    "    \n",
    "    return np.mean(emb, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(description, user):\n",
    "  return np.dot(description, user)/(np.linalg.norm(description)*np.linalg.norm(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_to_user(file_name):\n",
    "    df = concat_data(file_name)\n",
    "    user = user_profile(file_name)\n",
    "\n",
    "    emb = df['sentence_embeddings'].tolist()\n",
    "    cos_sim_list = []\n",
    "    for i in range(len(df)):\n",
    "        res = cos_sim(emb[i], user)\n",
    "        cos_sim_list.append(res)\n",
    "    df['cos_sim'] = cos_sim_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# уже на этом этапе можно посмотреть рекомендации, основанные только на косинусном подобии работ и профиля пользователя\n",
    "\n",
    "df = similarity_to_user('Relax_Iam_Exodium_animes')\n",
    "df_not_watched = df[df.user_status == 'not_watched']\n",
    "df_not_watched.sort_values(by=['cos_sim'], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## предсказание оценки пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'tuominn_animes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = similarity_to_user(file_name)\n",
    "df = df.drop(df[(df['start_date'] == 0) & (df['end_date'] == 0)].index)\n",
    "train = df[df.user_status != 'not_watched']\n",
    "predict = df[df.user_status == 'not_watched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['user_score']\n",
    "\n",
    "X_train = train[['cos_sim']]\n",
    "X_predict = predict[['cos_sim']]\n",
    "\n",
    "X_train_not_enc = train[['type', 'age_rating', 'genres', 'studio']]\n",
    "X_predict_not_enc = predict[['type', 'age_rating', 'genres', 'studio']]\n",
    "\n",
    "X_train_not_scaled = train[['ep', 'ep_len', 'start_date', 'rating']]\n",
    "X_predict_not_scaled = predict[['ep', 'ep_len', 'start_date', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot.fit(X_train_not_enc)\n",
    "X_train_enc = one_hot.transform(X_train_not_enc)\n",
    "X_test_enc = one_hot.transform(X_predict_not_enc)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_not_scaled)\n",
    "X_train_scaled = scaler.transform(X_train_not_scaled)\n",
    "X_predict_scaled = scaler.transform(X_predict_not_scaled)\n",
    "\n",
    "X_train_transformed = np.concatenate([X_train, X_train_enc.todense(), X_train_scaled], axis=1)\n",
    "X_predict_transformed = np.concatenate([X_predict, X_test_enc.todense(), X_predict_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_predict_tensor = torch.tensor(X_predict_transformed, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NonLinearRegressionModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_transformed.shape[1]\n",
    "hidden_dim = input_dim * 2\n",
    "output_dim = 1\n",
    "num_epochs = 100\n",
    "\n",
    "model = NonLinearRegressionModel(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, optimizer, train_loader, num_epochs):\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        losses.append(running_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_model(model, loss_function, optimizer, train_loader, num_epochs))\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_predict_tensor).cpu().numpy()\n",
    "\n",
    "predicted_scores = pd.DataFrame(predictions, columns=['predicted_score'])\n",
    "predict['predicted_score'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict.sort_values(by=['predicted_score'], ascending=False)[:10]\n",
    "res['name_ru'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.sort_values(by=['predicted_score'], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.sort_values(by=['predicted_score'])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict.sort_values(by=['predicted_score'], ascending=False)[:10]\n",
    "res = res.drop(res[['description', 'link', 'genres_description', 'sentence_embeddings', 'user_status', 'user_score']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## выдача рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**вариант 1:** рекомендации с учетом косинусного подобия, предсказанной оценки и оценки с сайта; в рекомендации попадает только то, что похоже на пользователя минимум на заданное значение косинусного подобия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(input('Сколько рекомендаций вы хотите получить?'))\n",
    "\n",
    "prediction_scaled = scaler.fit_transform(predict[['rating', 'predicted_score', 'cos_sim']])\n",
    "prediction_mean = np.mean(prediction_scaled, axis=1)\n",
    "predict['prediction'] = prediction_mean\n",
    "\n",
    "cos_sim_input = float(input('Введите минимальное допустимое значение похожести работ на уже просмотренные вами (от 0 до 1):'))\n",
    "rec = predict[predict['cos_sim'] >= cos_sim_input]\n",
    "rec.sort_values(by=['prediction'], ascending=False)[:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**вариант 2:** рекомендации с учетом косинусного подобия и предсказанной оценки с фильтрацией по оценке с сайта (можно задать минимально допустимую оценку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(input('Сколько рекомендаций вы хотите получить?'))\n",
    "\n",
    "prediction_scaled = scaler.fit_transform(predict[['predicted_score', 'cos_sim']])\n",
    "prediction_mean = np.mean(prediction_scaled, axis=1)\n",
    "predict['prediction'] = prediction_mean\n",
    "\n",
    "rating_input = int(input('Введите минимальное допустимое значение рейтинга:'))\n",
    "rec = predict[predict['rating'] >= rating_input]\n",
    "rec.sort_values(by=['prediction'], ascending=False)[:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**вариант 3:** самый гибкий и настраиваемый, но требующий больше всего решений от пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - предсказанная оценка пользователя\n",
    "# 2 - рейтинг с сайта\n",
    "# 3 - ничего из этого (рекомендации будут выданы исходя только из косинусного подобия работ и профиля пользователя)\n",
    "\n",
    "num = int(input('Сколько рекомендаций вы хотите получить?'))\n",
    "param_input = [int(i) for i in input('Выберите параметры, которые хотите учитывать для выдачи рекомендаций. Введите цифры через проблел:').strip(' ').split(' ')]\n",
    "rating_input = 0\n",
    "\n",
    "rec_param = ['cos_sim']\n",
    "for j in param_input:\n",
    "    if j == 1:\n",
    "        rec_param.append('predicted_score')\n",
    "    elif j == 2:\n",
    "        rec_param.append('rating')\n",
    "        rating_input = int(input('Введите минимальное допустимое значение рейтинга:'))\n",
    "\n",
    "if 3 not in param_input:\n",
    "    prediction_scaled = scaler.fit_transform(predict[rec_param])\n",
    "    prediction_mean = np.mean(prediction_scaled, axis=1)\n",
    "    predict['prediction'] = prediction_mean\n",
    "    cos_sim_input = float(input('Введите минимальное допустимое значение похожести работ на уже просмотренные вами (от 0 до 1):'))\n",
    "    if 2 in param_input:\n",
    "        rec = predict[(predict['cos_sim'] >= cos_sim_input) & (predict['rating'] >= rating_input)].sort_values(by=['prediction'], ascending=False)[:num]\n",
    "    else:\n",
    "        rec = predict[predict['cos_sim'] >= cos_sim_input].sort_values(by=['prediction'], ascending=False)[:num]\n",
    "else:\n",
    "    rec = predict.sort_values(by=['cos_sim'], ascending=False)[:num]\n",
    "\n",
    "\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**фильтрация по жанрам**: в рекомендации попадают только работы с заданными жанрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scaled = scaler.fit_transform(predict[['rating', 'predicted_score', 'cos_sim']])\n",
    "prediction_mean = np.mean(prediction_scaled, axis=1)\n",
    "predict['prediction'] = prediction_mean\n",
    "\n",
    "\n",
    "rec = predict[predict['cos_sim'] >= cos_sim_input]\n",
    "cos_sim_input = float(input('Введите минимальное допустимое значение похожести работ на уже просмотренные вами (от 0 до 1):'))\n",
    "filter_genres = input('Введите жанры через запятую:')\n",
    "\n",
    "def contains_genre(list, filter_input):\n",
    "    if ',' in filter_input:\n",
    "        input_list = [word.strip() for word in filter_input.split(',')]\n",
    "        return all(word in list for word in input_list)\n",
    "    else:    \n",
    "        return filter_input in list\n",
    "\n",
    "filtered_df = rec[rec['genres'].apply(contains_genre, filter_input=filter_genres)]\n",
    "filtered_df.sort_values(by=['prediction'], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## поиск по описанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anime_info_emb.csv')\n",
    "df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "df['sentence_embeddings'] = df['sentence_embeddings'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# описания чего-то конкретного\n",
    "\n",
    "#text = 'аниме про японского школьника, который убивал преступников с помощью тетради, которую ему дал бог смерти, и детектива, который пытался его поймать'\n",
    "#text = 'аниме про двух братьев, которые начали заниматься алхимией, чтобы воскресить маму, но в итоге один потерял все тело, а второй ногу и руку'\n",
    "#text = 'аниме про клуб бегунов, которые собрались пробежать марафон'\n",
    "text = 'аниме про четырех друзей-школьников. просто повседневность комедия про школу'\n",
    "\n",
    "check = sentence_embeddings(text)\n",
    "\n",
    "df['result'] = df['sentence_embeddings'].apply(cos_sim, user=check)\n",
    "df.sort_values(by=['result'], ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# описания с предпочтениями, можно снова задать минимальную допустимую оценку\n",
    "\n",
    "text = 'аниме про спортивные соревнования'\n",
    "#text = 'повседневность иясикэй про работу'\n",
    "#text = 'повседневность комедия про школу'\n",
    "#text = 'повседневность комедия про школу без фэнтези'\n",
    "\n",
    "check = sentence_embeddings(text)\n",
    "\n",
    "num = int(input())\n",
    "df_emb = df[df['rating'] >= num]\n",
    "\n",
    "df_emb['result'] = df_emb['sentence_embeddings'].apply(cos_sim, user=check)\n",
    "df_emb.sort_values(by=['result'], ascending=False)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
